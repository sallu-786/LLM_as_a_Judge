# LLM_as_a_Judge

A framework that uses one LLM instance to evaluate responses generated by another LLM, automating the quality assessment process.

## How It Works
1. Take responses generated by an LLM
2. Sends them to a "judge" LLM along with ground truth(actual answer)
3. Judge evaluates responses for accuracy and quality
4. Provides scores and reasoning for each evaluation

   ![image](https://github.com/user-attachments/assets/c4f350a1-748c-430c-9cf0-2ef25d9ee7fd)


## Installation

### Prerequisites
- LLM API or Ollama installed and running (if running on local machine)
- Python 3.11+

### Required Libraries
```bash
pip install -r requirements.txt
```

## Project Structure
```
├── utils/
│   ├── build_prompt.py    # Prompt engineering
│   ├── evaluate.py        # Evaluation logic
│   └── plots.py          # Visualization tools
└── main.py               # Example usage
```

## Output
- Binary classification (YES/NO)
- Evaluation reasoning
- Visualization of results (pie charts, distribution plots, confusion matrices)

## License
MIT Licencse
